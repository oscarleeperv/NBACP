{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c927612-65cd-4640-b691-295d903b281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#statistical testing tools\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "\n",
    "#regression models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn import linear_model\n",
    "from sklearn import kernel_ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c2e84a-e8f0-494c-991c-605a0a780e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data_ready.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13abc797-5c38-4510-b90e-bca496b313f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_inputs = ['COL_GP', 'COL_PTS_PERG', 'COL_REB_PERG', 'COL_AST_PERG',\n",
    "             'COL_FG_PCT', 'COL_FG3_PCT', 'COL_FT_PCT']\n",
    "model_outputs = ['PRO_MIN_PERG', 'PRO_PTS_PERG', 'PRO_AST_PERG',\n",
    "                 'PRO_REB_PERG', 'PRO_FG_PCT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "889da266-c420-4325-9ed0-14223cf6c081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>PRO_AST_PERG</td>   <th>  R-squared (uncentered):</th>      <td>   0.624</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared (uncentered):</th> <td>   0.619</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>          <td>   118.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 07 Aug 2024</td> <th>  Prob (F-statistic):</th>          <td>4.95e-102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>17:23:35</td>     <th>  Log-Likelihood:    </th>          <td> -741.30</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   507</td>      <th>  AIC:               </th>          <td>   1497.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   500</td>      <th>  BIC:               </th>          <td>   1526.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>              <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>COL_GP</th>       <td>   -0.0067</td> <td>    0.011</td> <td>   -0.615</td> <td> 0.539</td> <td>   -0.028</td> <td>    0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>COL_PTS_PERG</th> <td>    0.0071</td> <td>    0.015</td> <td>    0.493</td> <td> 0.623</td> <td>   -0.021</td> <td>    0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>COL_REB_PERG</th> <td>   -0.0472</td> <td>    0.028</td> <td>   -1.695</td> <td> 0.091</td> <td>   -0.102</td> <td>    0.008</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>COL_AST_PERG</th> <td>    0.3567</td> <td>    0.033</td> <td>   10.877</td> <td> 0.000</td> <td>    0.292</td> <td>    0.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>COL_FG_PCT</th>   <td>    1.9222</td> <td>    0.887</td> <td>    2.168</td> <td> 0.031</td> <td>    0.180</td> <td>    3.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>COL_FG3_PCT</th>  <td>   -0.2791</td> <td>    0.785</td> <td>   -0.356</td> <td> 0.722</td> <td>   -1.822</td> <td>    1.263</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>COL_FT_PCT</th>   <td>   -0.2107</td> <td>    0.542</td> <td>   -0.389</td> <td> 0.698</td> <td>   -1.276</td> <td>    0.855</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>206.373</td> <th>  Durbin-Watson:     </th> <td>   2.116</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>1018.750</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.750</td>  <th>  Prob(JB):          </th> <td>6.04e-222</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 8.998</td>  <th>  Cond. No.          </th> <td>    762.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] R² is computed without centering (uncentered) since the model does not contain a constant.<br/>[2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &  PRO\\_AST\\_PERG  & \\textbf{  R-squared (uncentered):}      &     0.624   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared (uncentered):} &     0.619   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       }          &     118.7   \\\\\n",
       "\\textbf{Date:}             & Wed, 07 Aug 2024 & \\textbf{  Prob (F-statistic):}          & 4.95e-102   \\\\\n",
       "\\textbf{Time:}             &     17:23:35     & \\textbf{  Log-Likelihood:    }          &   -741.30   \\\\\n",
       "\\textbf{No. Observations:} &         507      & \\textbf{  AIC:               }          &     1497.   \\\\\n",
       "\\textbf{Df Residuals:}     &         500      & \\textbf{  BIC:               }          &     1526.   \\\\\n",
       "\\textbf{Df Model:}         &           7      & \\textbf{                     }          &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     }          &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                        & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{COL\\_GP}        &      -0.0067  &        0.011     &    -0.615  &         0.539        &       -0.028    &        0.015     \\\\\n",
       "\\textbf{COL\\_PTS\\_PERG} &       0.0071  &        0.015     &     0.493  &         0.623        &       -0.021    &        0.036     \\\\\n",
       "\\textbf{COL\\_REB\\_PERG} &      -0.0472  &        0.028     &    -1.695  &         0.091        &       -0.102    &        0.008     \\\\\n",
       "\\textbf{COL\\_AST\\_PERG} &       0.3567  &        0.033     &    10.877  &         0.000        &        0.292    &        0.421     \\\\\n",
       "\\textbf{COL\\_FG\\_PCT}   &       1.9222  &        0.887     &     2.168  &         0.031        &        0.180    &        3.664     \\\\\n",
       "\\textbf{COL\\_FG3\\_PCT}  &      -0.2791  &        0.785     &    -0.356  &         0.722        &       -1.822    &        1.263     \\\\\n",
       "\\textbf{COL\\_FT\\_PCT}   &      -0.2107  &        0.542     &    -0.389  &         0.698        &       -1.276    &        0.855     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 206.373 & \\textbf{  Durbin-Watson:     } &     2.116  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } &  1018.750  \\\\\n",
       "\\textbf{Skew:}          &   1.750 & \\textbf{  Prob(JB):          } & 6.04e-222  \\\\\n",
       "\\textbf{Kurtosis:}      &   8.998 & \\textbf{  Cond. No.          } &      762.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] R² is computed without centering (uncentered) since the model does not contain a constant. \\newline\n",
       " [2] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                                 OLS Regression Results                                \n",
       "=======================================================================================\n",
       "Dep. Variable:           PRO_AST_PERG   R-squared (uncentered):                   0.624\n",
       "Model:                            OLS   Adj. R-squared (uncentered):              0.619\n",
       "Method:                 Least Squares   F-statistic:                              118.7\n",
       "Date:                Wed, 07 Aug 2024   Prob (F-statistic):                   4.95e-102\n",
       "Time:                        17:23:35   Log-Likelihood:                         -741.30\n",
       "No. Observations:                 507   AIC:                                      1497.\n",
       "Df Residuals:                     500   BIC:                                      1526.\n",
       "Df Model:                           7                                                  \n",
       "Covariance Type:            nonrobust                                                  \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "COL_GP          -0.0067      0.011     -0.615      0.539      -0.028       0.015\n",
       "COL_PTS_PERG     0.0071      0.015      0.493      0.623      -0.021       0.036\n",
       "COL_REB_PERG    -0.0472      0.028     -1.695      0.091      -0.102       0.008\n",
       "COL_AST_PERG     0.3567      0.033     10.877      0.000       0.292       0.421\n",
       "COL_FG_PCT       1.9222      0.887      2.168      0.031       0.180       3.664\n",
       "COL_FG3_PCT     -0.2791      0.785     -0.356      0.722      -1.822       1.263\n",
       "COL_FT_PCT      -0.2107      0.542     -0.389      0.698      -1.276       0.855\n",
       "==============================================================================\n",
       "Omnibus:                      206.373   Durbin-Watson:                   2.116\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1018.750\n",
       "Skew:                           1.750   Prob(JB):                    6.04e-222\n",
       "Kurtosis:                       8.998   Cond. No.                         762.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
       "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[sr_inputs]\n",
    "y = df['PRO_AST_PERG']\n",
    "model = sm.OLS(y, X)\n",
    "lm = model.fit()\n",
    "lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8af762-ab6b-4476-bfdc-3ce2a15e3775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:0.20236226830782894\n",
      "std:0.1799034307028945\n"
     ]
    }
   ],
   "source": [
    "#ridge alpha trial and error\n",
    "X = df[['COL_FG_PCT', 'COL_AST_PERG']]\n",
    "y = df['PRO_AST_PERG']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "model = linear_model.Ridge(alpha=0.4)\n",
    "scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "print(f'mean:{scores.mean()}\\nstd:{scores.std()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51941f6f-5549-4b7c-8c20-4273ae5e998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(y_test, pred):\n",
    "    mse = mean_squared_error(y_test, pred)\n",
    "    return math.sqrt(mse)\n",
    "\n",
    "#brute-force regressional model testing \n",
    "def get_scores(X, y):\n",
    "    get_linear_regression(X, y)\n",
    "    get_binomial_regression(X, y)\n",
    "    get_decision_tree_regressor(X, y)\n",
    "    get_random_forest_regressor(X, y)\n",
    "    get_ridge_regression(X, y)\n",
    "    get_lasso_regression(X, y)\n",
    "    get_orthogonal_mp(X, y)\n",
    "    get_kernel_ridge_regression(X, y)\n",
    "    get_svr_regression(X, y)\n",
    "    get_nusvr_regression(X, y)\n",
    "    get_linear_svr_regression(X, y)\n",
    "    \n",
    "def get_linear_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = LinearRegression()\n",
    "    model_type = 'Linear Regression'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_binomial_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = LinearRegression()\n",
    "    poly_features = PolynomialFeatures(degree=2)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    model_type = 'Binomial Regression'\n",
    "    score = cross_val_score(model, X_train_poly, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train_poly, y_train)\n",
    "    X_test_poly = poly_features.fit_transform(X_test)\n",
    "    pred = model.predict(X_test_poly)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_decision_tree_regressor(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = DecisionTreeRegressor()\n",
    "    model_type = 'Decision Tree Regressor'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_random_forest_regressor(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = RandomForestRegressor()\n",
    "    model_type = 'Random Forest Regressor'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_ridge_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = linear_model.Ridge(alpha=1)\n",
    "    model_type = 'Ridge Regression'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_lasso_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = linear_model.Lasso(alpha=1)\n",
    "    model_type = 'Lasso Regression'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_orthogonal_mp(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = linear_model.OrthogonalMatchingPursuit()\n",
    "    model_type = 'Orthogonal MP'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_kernel_ridge_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = kernel_ridge.KernelRidge()\n",
    "    model_type = 'Kernel Ridge Regression'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_svr_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = SVR()\n",
    "    model_type = 'SVR Regression'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_nusvr_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = NuSVR()\n",
    "    model_type = 'NuSVR Regression'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')\n",
    "\n",
    "def get_linear_svr_regression(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    model = LinearSVR()\n",
    "    model_type = 'Linear SVR Regression'\n",
    "    score = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    rmse = get_rmse(y_test, pred)\n",
    "    print(f'{model_type}:\\nrmse:{rmse}\\nmean:{score.mean()}\\nstd:{score.std()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "275380fc-918b-4012-a783-aa71c5efadab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression:\n",
      "rmse:0.9977595377988783\n",
      "mean:0.1995816620720817\n",
      "std:0.18214123848784944\n",
      "\n",
      "Binomial Regression:\n",
      "rmse:1.0009485446305044\n",
      "mean:0.23435519706014668\n",
      "std:0.20778373256767613\n",
      "\n",
      "Decision Tree Regressor:\n",
      "rmse:1.436957367886808\n",
      "mean:-1.0037600281862313\n",
      "std:0.9421353946281784\n",
      "\n",
      "Random Forest Regressor:\n",
      "rmse:1.061116172372558\n",
      "mean:-0.05090464266225857\n",
      "std:0.2744836456660774\n",
      "\n",
      "Ridge Regression:\n",
      "rmse:0.9931801531307967\n",
      "mean:0.20334045945672163\n",
      "std:0.17855678972600655\n",
      "\n",
      "Lasso Regression:\n",
      "rmse:0.9890183310762248\n",
      "mean:0.022967984861749537\n",
      "std:0.04017102664858251\n",
      "\n",
      "Orthogonal MP:\n",
      "rmse:0.9915232342511312\n",
      "mean:0.2025672733822897\n",
      "std:0.17695440891182126\n",
      "\n",
      "Kernel Ridge Regression:\n",
      "rmse:0.9890995724088012\n",
      "mean:0.20622932340079903\n",
      "std:0.17623865839814135\n",
      "\n",
      "SVR Regression:\n",
      "rmse:1.0104022182615573\n",
      "mean:0.1565580964440485\n",
      "std:0.14934340718174102\n",
      "\n",
      "NuSVR Regression:\n",
      "rmse:0.9876606074534141\n",
      "mean:0.19213002160554218\n",
      "std:0.1471340486669456\n",
      "\n",
      "Linear SVR Regression:\n",
      "rmse:1.000554379579581\n",
      "mean:0.17615929567277436\n",
      "std:0.126549851371576\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\oscar_5nxc3x5\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#function to see how each model with different independent and dependent variables\n",
    "get_scores(df[['COL_FG_PCT', 'COL_AST_PERG']], df['PRO_AST_PERG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b464dcf6-f8ef-465c-af0d-2267215c0b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
